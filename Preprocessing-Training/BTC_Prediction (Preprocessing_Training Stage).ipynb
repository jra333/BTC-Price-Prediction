{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTC Price Prediction (Preprocessing and Training)\n",
    "\n",
    "### This notebook contains:\n",
    "- Use of \"Standard Scaler\" to ensure values are to scale for modeling (no large fluctuations), preventing leakage\n",
    "- Splitting of dataframe into \"testing\" and \"training\" subsets using \"train_test_split\" \n",
    "- Overall preprocessing to prep for modeling and implementation of various classifying/tree based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('data/train.csv', index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ema_short</th>\n",
       "      <th>ema_long</th>\n",
       "      <th>atr</th>\n",
       "      <th>obv</th>\n",
       "      <th>tweet_sentiment</th>\n",
       "      <th>close_nextday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-09-02</th>\n",
       "      <td>10340.00</td>\n",
       "      <td>44740.25</td>\n",
       "      <td>10164.518939</td>\n",
       "      <td>10452.265343</td>\n",
       "      <td>530.693553</td>\n",
       "      <td>225053.863244</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10615.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03</th>\n",
       "      <td>10615.28</td>\n",
       "      <td>47998.38</td>\n",
       "      <td>10207.448563</td>\n",
       "      <td>10458.658074</td>\n",
       "      <td>528.572585</td>\n",
       "      <td>273052.240025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10567.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-04</th>\n",
       "      <td>10567.02</td>\n",
       "      <td>43943.89</td>\n",
       "      <td>10241.693462</td>\n",
       "      <td>10462.907561</td>\n",
       "      <td>521.468114</td>\n",
       "      <td>229108.350999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10564.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-05</th>\n",
       "      <td>10564.49</td>\n",
       "      <td>33970.96</td>\n",
       "      <td>10272.435990</td>\n",
       "      <td>10466.891187</td>\n",
       "      <td>516.363249</td>\n",
       "      <td>195137.390360</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10298.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-06</th>\n",
       "      <td>10298.73</td>\n",
       "      <td>58799.64</td>\n",
       "      <td>10274.940181</td>\n",
       "      <td>10460.296630</td>\n",
       "      <td>533.470874</td>\n",
       "      <td>136337.749401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10455.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               close    volume     ema_short      ema_long         atr  \\\n",
       "Date                                                                     \n",
       "2019-09-02  10340.00  44740.25  10164.518939  10452.265343  530.693553   \n",
       "2019-09-03  10615.28  47998.38  10207.448563  10458.658074  528.572585   \n",
       "2019-09-04  10567.02  43943.89  10241.693462  10462.907561  521.468114   \n",
       "2019-09-05  10564.49  33970.96  10272.435990  10466.891187  516.363249   \n",
       "2019-09-06  10298.73  58799.64  10274.940181  10460.296630  533.470874   \n",
       "\n",
       "                      obv  tweet_sentiment  close_nextday  \n",
       "Date                                                       \n",
       "2019-09-02  225053.863244             -1.0       10615.28  \n",
       "2019-09-03  273052.240025              0.5       10567.02  \n",
       "2019-09-04  229108.350999              0.5       10564.49  \n",
       "2019-09-05  195137.390360              0.5       10298.73  \n",
       "2019-09-06  136337.749401              0.0       10455.88  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls\n",
    "\n",
    "print(pd.isnull(training_data).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "df_scaled = pd.DataFrame(ss.fit_transform(training_data),\n",
    "                         index=training_data.index,\n",
    "                         columns=training_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe split (training and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_scaled.drop(labels=['close_nextday'], axis=1)\n",
    "y = df_scaled['close_nextday']\n",
    "#X = sm.add_constant(X)\n",
    "\n",
    "print(X.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression Using OLS (Ordinary Least Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = sm.OLS(y_train, X_train)\n",
    "\n",
    "results = lr.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary results, we can see that the Adj. R-Squared was 0.970. This shows us that the model explains it variability fairly well. However, this metric solely does not explain the overall performance of the model but it does show us that our predictors are on the right track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = results.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(y_test, y_pred)\n",
    "_ = plt.plot([x for x in range(-3, 6)], [y for y in range(-3, 6)], color='red')\n",
    "\n",
    "_ = plt.title('Model Prediction vs Actual')\n",
    "_ = plt.xlabel('actual values')\n",
    "_ = plt.ylabel('predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_test, y_pred):\n",
    "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Results of sklearn.metrics:\\n\")\n",
    "print(\"MAPE:\", mape)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 6))\n",
    "_ = plt.plot(y_test)\n",
    "_ = plt.plot(y_pred)\n",
    "_ = plt.legend([\"actual\", \"pred\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding redundancy in linear regression models\n",
    "\n",
    "Since the feature \"close\" is already found to be correlated with our prediction of \"close_nextday\" (from heatmap EDA stage), we can remove \"close\" to see how disposable it really is in relation to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in correlation data gathered from EDA correlation heatmap\n",
    "\n",
    "f = open('data/highest_corr_target.txt', 'r')\n",
    "file_contents = f.read()\n",
    "print('Highest correlated variable with target:\\n', \"\\n\", file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df_scaled.drop(labels=['close_nextday', 'close'], axis=1)\n",
    "y2 = df_scaled['close_nextday']\n",
    "#X2 = sm.add_constant(X2)\n",
    "print(X2.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2,\n",
    "                                                        y2,\n",
    "                                                        test_size=0.25,\n",
    "                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = sm.OLS(y2_train, X2_train)\n",
    "\n",
    "results2 = lr2.fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the results of our summary statistics give us an Adj. R-squared measure of 0.907. This was an expected drop from our last model as we removed one of our predictors (\"close\"), which is a highly correlated variable to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = results2.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(y2_test, y2_pred)\n",
    "_ = plt.plot([x for x in range(-3, 6)], [y for y in range(-3, 6)], color='red')\n",
    "\n",
    "_ = plt.title('Model 2 Prediction vs Actual')\n",
    "_ = plt.xlabel('actual values')\n",
    "_ = plt.ylabel('predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y2_test, y2_pred):\n",
    "    y2_test, y2_pred = np.array(y2_test), np.array(y2_pred)\n",
    "    return np.mean(np.abs((y2_test - y2_pred) / y2_test)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape2 = mean_absolute_percentage_error(y2_test, y2_pred)\n",
    "mae2 = metrics.mean_absolute_error(y2_test, y2_pred)\n",
    "mse2 = metrics.mean_squared_error(y2_test, y2_pred)\n",
    "rmse2 = np.sqrt(mse)\n",
    "\n",
    "print(\"Results of sklearn.metrics:\\n\")\n",
    "print(\"MAPE:\", mape2)\n",
    "print(\"MAE:\", mae2)\n",
    "print(\"MSE:\", mse2)\n",
    "print(\"RMSE:\", rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 6))\n",
    "_ = plt.plot(y2_test)\n",
    "_ = plt.plot(y2_pred)\n",
    "_ = plt.legend([\"actual\", \"pred\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.to_csv('data/btc_df_scaled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Findings and Conclusion\n",
    "\n",
    "During this stage (preprocessing and training), we started by standardizing the magnitude of numeric features within our original dataframe by using _StandardScaler()_. This function allows us to normalize the features of each column(X), individually, so that each column/feature/variable will have a mean of 0 and standard deviation of 1. After transforming the dataframe we then split it into two separate ones, training and testing using _train_test_split()_. From there we created a baseline for our predictions or future models by utilizing two simple multi linear regression models using ordinary least squares(OLS). \n",
    "\n",
    "One iteration of our linear regression model, for X contained all available features, aside from the target(y). While the other one for X contained all features except the target and \"close\", with the target(y) remaining the same. We then calculated the _MAPE_, _MAE_, and _MSE_ for each. By comparison, the first linear regression model performed better as the mean absolute percent error was roughly 53%, meaning the errors are \"slightly greater\" than the actual values. Whereas the second linear regression model gave us an _MAPE_ of roughly 198%, meaning our errors of that model were \"much greater\" than the actual values. This difference in _MAPE_ can be seen as well when comparing _RMSE_ results to _MAE_ results for each model. We can see the first model gave an _RMSE_ output of approx. 0.13 and an _MAE_ output of approx. 0.096. Since the two measurements are relatively close to each other it can be implied that the model makes many mistakes, but the mistakes are \"small\". The same can be said for the other model but opposite when comparing metrics. \n",
    "\n",
    "That being said, we can see that the feature \"close\" is fairly important when being utilized in a regression model. As when it was removed, the performance of the model decreased. Because of these brief findings, we have further confirmed the original \"hypothesis\" formed during the EDA stage in exploring the high correlation between \"close_nextday\" and \"close\".\n",
    "\n",
    "During the next stage (modeling), we will be examining other model architecture such as ensemble methods through \"trees\", i.e. random forest regressors(RFR). We will be improving or tuning our different models using optimization methods like _GridSearchCV_. Moreover, optimization will also aid us in the analysis of features (i.e. feature \"impact\" or \"importance\") as well as interpretability as a whole. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
